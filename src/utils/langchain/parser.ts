'use server';

import type { Document } from 'langchain/document';

import { TokenTextSplitter, RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

import { DocumentLang } from '@/models/Document';
import { logError, logInfo } from '@/utils/logger';

import { getDocumentLoader } from './loader';

// Langchain Document real types, for per text segment
export type LSegment = {
    metadata: {
        category: string;
        filename: string;
        filetype: string;
        languages: string[];
        loc: object;
        orig_elements: string;
        emphasized_text_contents?: string[];
        emphasized_text_tags?: string[];
    };
    pageContent: string;
};

export type DocumentMeta = {
    title: string;
    description: string;
    keywords: string[];
    lang: DocumentLang; // main language
    authors?: string[]; //original authors
    coverUrl?: string;
};

export type ParsedResult = {
    state: boolean;
    meta?: DocumentMeta;
    segments?: LSegment[];
};

export async function getSplitContents(filepath: string, extName: string) {
    try {
        console.time('ğŸ•° loadAndTokenTextSplit costs:');
        const docs: Document[] = await getDocumentLoader(filepath, extName).load();
        const splitDocs = new RecursiveCharacterTextSplitter({
            chunkSize: 3072,
            chunkOverlap: 200,
            separators: ['|', '##', '>', '-'],
        }).splitDocuments(docs);
        return splitDocs;
    } catch (error: any) {
        logError('âŒ DocumentLoader exception: ', error?.message, ', cause is: ', error?.cause);
    } finally {
        console.timeEnd('ğŸ•° loadAndTokenTextSplit costs:');
    }
    return null;
}

export async function parseFileContent(filePath: string, extName: string): Promise<ParsedResult> {
    try {
        const segments = (await getSplitContents(filePath, extName)) as LSegment[];
        if (Array.isArray(segments) && segments.length > 0) {
            // æ ‡é¢˜å’Œæè¿°æš‚æ—¶å‡ä»ç¬¬ä¸€èŠ‚å†…å®¹æˆªå–
            const firstParts = segments[0].pageContent.split('\n\n');
            logInfo('File firstPart:\n', firstParts);

            let title = '',
                description = '',
                keywords = [''];
            if (firstParts.length > 0) {
                title = firstParts[0].replace(/(#|\*|%|@|\$|&|-{2,})/g, '').substring(0, 128);
                description = firstParts
                    .join('')
                    .replace(/(#|\*|%|@|\$|&|-{2,})/g, '')
                    .substring(0, 255);
                // æš‚æ—¶ç›´æ¥åˆ‡å‰²æ ‡é¢˜
                keywords = title.split(' ');
            }

            // è¿”å›å®é™…çš„å†…å®¹æ•°æ®è½åº“ï¼Œä»¥ä¾¿å‰åå°ç»™ç”¨æˆ·å±•ç¤º
            return {
                state: true,
                meta: {
                    title,
                    description,
                    keywords,
                    lang: segments[0].metadata.languages.length ? (segments[0].metadata.languages[0].toUpperCase() as DocumentLang) : DocumentLang.ENG,
                    authors: ['tomartisan'], // å…ˆå†™æ­»ï¼Œåé¢ä»å‰ç«¯ä¼ è¿‡æ¥ã€‚æˆ–è€…ä»ç½‘ç»œæŠ“å–
                    coverUrl: process.env.__RSN_DEFAULT_COVER, // åè¾¹ä»ç½‘ç»œæŠ“å–ï¼Œæˆ–éšæœº
                },
                segments,
            };
        }
    } catch (error) {
        logError('parseFileContent', error);
    }
    return { state: false };
}
